{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the required Python Modules (and modules created for BinMSGUI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import sys\n",
    "import struct\n",
    "import json\n",
    "import datetime\n",
    "import scipy\n",
    "import scipy.signal\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# this will obtain the path for the notebook\n",
    "# if you have moved the notebook, you will need to set the notebook_path to the\n",
    "# .../BinMSGUI/BinMSGUI/jupyter directory\n",
    "notebook_path = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "\n",
    "# the code path is two folders up from this notebook + /code\n",
    "core_path = os.path.dirname(notebook_path)\n",
    "basepath = os.path.dirname(os.path.dirname(notebook_path))\n",
    "\n",
    "# add the core_path and basepath so we can import core.module_name\n",
    "sys.path.append(core_path)\n",
    "sys.path.append(basepath)\n",
    "\n",
    "# import the binMSGUI modules\n",
    "from core.readMDA import readMDA, get_Fs\n",
    "from core.readBin import get_bin_data, get_raw_pos, get_active_tetrode, get_channel_from_tetrode, get_active_eeg\n",
    "from core.Tint_Matlab import int16toint8, get_setfile_parameter\n",
    "from core.tetrode_conversion import convert_tetrode, batch_basename_tetrodes\n",
    "from core.convert_position import convert_position\n",
    "from core.eeg_conversion import convert_eeg\n",
    "from core.utils import find_sub, find_bin_basenames\n",
    "from core.bin2mda import convert_bin2mda\n",
    "from core.mdaSort import sort_bin, run_sort\n",
    "from core.set_conversion import convert_setfile\n",
    "from core.intan_mountainsort import validate_session, convert_bin_mountainsort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose Directory Containing .Bin Files to Analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 basename(s) within this directory: /mnt/d/freelance-work/catalyst-neuro/hussaini-lab-to-nwb/example_data_raw/data_5s\n",
      "------------------------\n",
      "axona_raw_5s\n"
     ]
    }
   ],
   "source": [
    "# directory that you want to analyze\n",
    "directory = r'/mnt/d/freelance-work/catalyst-neuro/hussaini-lab-to-nwb/example_data_raw/data_5s'\n",
    "\n",
    "# finds the basenames within this directory\n",
    "basenames = find_bin_basenames(directory)\n",
    "\n",
    "# prints all the basenames\n",
    "print('Found %d basename(s) within this directory: %s' % (len(basenames), directory))\n",
    "print('------------------------')\n",
    "for name in basenames:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Parameters\n",
    "This cell will contain all the parameters that are required for the analysis. There are quite a few so make sure that you check that the parameters are correct before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### filtering parameters ###########\n",
    "# The .bin data is already filtered based upon whatever settings you chose in the dacqUSB, therefore this\n",
    "# is just for show and does not do anything. If you want me to add an actual filtering step feel free to\n",
    "# let me know, right now this is just for show.\n",
    "\n",
    "freq_min = 300  # -units Hz- default 300\n",
    "freq_max = 7000  # -units Hz- default 7000\n",
    "notch_filter = False  # the data is already notch filtered likely\n",
    "\n",
    "########## Clip Parameters ##############\n",
    "pre_spike = 15\n",
    "post_spike = 35\n",
    "clip_size = 50 # this needs to be left at 50 for Tint, Tint only likes 50 samples\n",
    "if (pre_spike + post_spike != clip_size) or (clip_size != 50):\n",
    "    raise Exception('The pre_spike and the post_spike must add up to be 50, and the clip size must remain at 50 for Tint!')\n",
    "\n",
    "############ whiten ####################\n",
    "# do you want to whiten the data? The MountainSort Paper suggests that spatial whitening is crucial\n",
    "# for separating nearby clusters since it will remove any correlations among channels. Keep in mind\n",
    "# that spatial whitening will also normalize your data so when you threshold you will be thresholding\n",
    "# based off of # of standard deviations, and not a bit/uV value.\n",
    "# if you want to whiten, set whiten='true', if you do not set whiten='false'\n",
    "whiten = 'true' \n",
    "# whiten = 'false'\n",
    "\n",
    "############# detect_sign ################\n",
    "# recommend only doing positive peaks so we don't get any weird issues with a cell that is\n",
    "# aligned with the peak, and seemingly the same cell aligned with the trough (in this case\n",
    "# both peak and trough would have to exceed the threshold).\n",
    "\n",
    "# detect_sign = 0  # positive or negative peaks\n",
    "detect_sign = 1  # only positive peaks\n",
    "# detect_sign = -1  # only negative peaks\n",
    "\n",
    "########## detect_interval ##############\n",
    "# the algorithm will take the detect_interval value and bin the data in bin sizes of that many\n",
    "# samples. Then it will find the peak (or trough, or both) of each bin and evaluate that event\n",
    "# if it exceeds the threshold value. Therefore the detect_interval is roughly the number of \n",
    "# samples between the events (peaks/troughs depending on your detect_sign)\n",
    "\n",
    "# default detect_interval is 50\n",
    "\n",
    "# detect_interval = 50 \n",
    "detect_interval = 20  \n",
    "\n",
    "############ detect_threshold ###############\n",
    "\n",
    "# threshold values, I changed it into a whitened and non whitened threshold\n",
    "# this is because if you whiten the data you normalize it by the variance, thus\n",
    "# a threshold of 3 is essentially saying 3 standard deviations. However if you do not whiten\n",
    "# the data is not normalized and thus, you would be using a bit value, maybe should take whatever\n",
    "# value is in the threshold from the set file.\n",
    "\n",
    "automate_threshold = False  # Don't Change this, here we are just initializing the automate_threshold value to False by default\n",
    "\n",
    "if whiten == 'true':\n",
    "    # detect_threshold = 3  # units: ~sd's\n",
    "    detect_threshold = 4  # units: ~sd's\n",
    "    \n",
    "else:\n",
    "    # this mean's the data was not whitened\n",
    "    \n",
    "    detect_threshold = 13000  #  units: bits \n",
    "    \n",
    "    # if you want to find the threshold from the .set file and use that \n",
    "    # set automate_threshold to True, otherwise False. This threshold would override any\n",
    "    # value set above. I'd recommend setting this to true as this is variable from .set file\n",
    "    # to .set file it seems.\n",
    "    # automate_threshold = True \n",
    "    automate_threshold = False\n",
    "\n",
    "# ########### artifact masking parameters ###########\n",
    "# here we bin the data into masked_chunk_size bins, and it will take the sqrt of the sum of \n",
    "# the squares (RSS) for each bin. It will then find the SD for all the bins, and if the bin is\n",
    "# above mask_threshold SD's from the average bin RSS, it will consider it as high amplitude noise\n",
    "# and remove this chunk (and neighboring chunks).\n",
    "\n",
    "# mask = True  # set the value to True if you want to include the artifact masking step\n",
    "mask = False  # set the value to False if you want to skip the artifact masking step\n",
    "mask_threshold = 6  #  units: SD's, the threshold that once exceed the bin/chunk will be zero'ed (and the neighbors)\n",
    "\n",
    "# the size of the bins/chunks to use when binning the data. If the value is set to  None \n",
    "# the defaul tmasked_chunk_size it will default to Fs/20\n",
    "masked_chunk_size = None  \n",
    "\n",
    "mask_num_write_chunks = 100  # how many chunks will be simultaneously written to the masked output file\n",
    "\n",
    "########## Feature/PCA Parameters ##########\n",
    "num_features = 10\n",
    "max_num_clips_for_pca = 1000\n",
    "\n",
    "# random parameters, probably don't need to change\n",
    "self = None  # don't worry about this, this is for objective oriented programming (my GUIs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runs Analysis on each Basename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/d/freelance-work/catalyst-neuro/hussaini-lab-to-nwb/example_data_raw/data_5s'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'axona_raw_5s'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.basename(current_basename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing set_file 1/1: \n",
      "Using the following detect_threshold: 4.00\n",
      "[2021-04-27 10:07:42]: Converting the following tetrode: 1!\n",
      "[2021-04-27 10:07:47]: Converting the following tetrode: 2!\n",
      "[2021-04-27 10:07:55]: Converting the following tetrode: 3!\n",
      "[2021-04-27 10:08:01]: Converting the following tetrode: 4!\n",
      "[2021-04-27 10:08:08]: Sorting the following file: /mnt/d/freelance-work/catalyst-neuro/hussaini-lab-to-nwb/example_data_raw/data_5s/axona_raw_5s_T1_filt.mda!\n",
      "ml-run-process ms4_geoff.sort --inputs filt_fname:/mnt/mnt/d/freelance-work/catalyst-neuro/hussaini-lab-to-nwb/example_data_raw/data_5s/axona_raw_5s_t1_filt.md/mnt/d/freelance-work/catalyst-neuro/hussaini-lab-to-nwb/example_data_raw/data_5s/axona_raw_5s_T1_filt.mda --outputs firings_out:/mnt/mnt/d/freelance-work/catalyst-neuro/hussaini-lab-to-nwb/example_data_raw/data_5s/axona_raw_5s_t1_firings.md/mnt/d/freelance-work/catalyst-neuro/hussaini-lab-to-nwb/example_data_raw/data_5s/axona_raw_5s_T1_firings.mda pre_out_fname:/mnt/mnt/d/freelance-work/catalyst-neuro/hussaini-lab-to-nwb/example_data_raw/data_5s/axona_raw_5s_t1_pre.md/mnt/d/freelance-work/catalyst-neuro/hussaini-lab-to-nwb/example_data_raw/data_5s/axona_raw_5s_T1_pre.mda metrics_out_fname:/mnt/mnt/d/freelance-work/catalyst-neuro/hussaini-lab-to-nwb/example_data_raw/data_5s/axona_raw_5s_t1_metrics.jso/mnt/d/freelance-work/catalyst-neuro/hussaini-lab-to-nwb/example_data_raw/data_5s/axona_raw_5s_T1_metrics.json --parameters freq_min:300 freq_max:7000 samplerate:48000 detect_sign:1 adjacency_radius:-1 detect_threshold:4 detect_interval:20 clip_size:50 firing_rate_thresh:0.05 isolation_thresh:0.95 noise_overlap_thresh:0.03 peak_snr_thresh:1.5 mask_artifacts:false mask_chunk_size:2400 mask_threshold:6 mask_num_write_chunks:100 num_workers:8 whiten:true num_features:10 max_num_clips_for_pca:1000 >> /mnt/mnt/d/freelance-work/catalyst-neuro/hussaini-lab-to-nwb/example_data_raw/data_5s/axona_raw_5s_t1_terminal.tx/mnt/d/freelance-work/catalyst-neuro/hussaini-lab-to-nwb/example_data_raw/data_5s/axona_raw_5s_T1_terminal.txt\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'MNT:\\\\d\\\\freelance-work\\\\catalyst-neuro\\\\hussaini-lab-to-nwb\\\\example_data_raw\\\\data_5s\\\\axona_raw_5s_t1_terminal.tx\\\\mnt\\\\d\\\\freelance-work\\\\catalyst-neuro\\\\hussaini-lab-to-nwb\\\\example_data_raw\\\\data_5s\\\\axona_raw_5s_T1_terminal.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-47706235772b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m                              \u001b[0mmax_num_clips_for_pca\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_num_clips_for_pca\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                              \u001b[0mpre_spike\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpre_spike\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpost_spike\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpost_spike\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                              notch_filter=notch_filter, self=self)\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-------------------'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/d/spikeinterface/BinMSGUI/BinMSGUI/core/intan_mountainsort.py\u001b[0m in \u001b[0;36mconvert_bin_mountainsort\u001b[0;34m(directory, tint_basename, whiten, detect_interval, detect_sign, detect_threshold, freq_min, freq_max, mask_threshold, masked_chunk_size, mask_num_write_chunks, clip_size, notch_filter, pre_spike, post_spike, mask, num_features, max_num_clips_for_pca, self, verbose)\u001b[0m\n\u001b[1;32m    256\u001b[0m              \u001b[0mmax_num_clips_for_pca\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_num_clips_for_pca\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m              \u001b[0mself\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m              verbose=verbose)\n\u001b[0m\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0;31m# create positions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/d/spikeinterface/BinMSGUI/BinMSGUI/core/mdaSort.py\u001b[0m in \u001b[0;36msort_bin\u001b[0;34m(directory, tint_fullpath, whiten, detect_interval, detect_sign, detect_threshold, freq_min, freq_max, mask_threshold, masked_chunk_size, mask_num_write_chunks, clip_size, mask, num_features, max_num_clips_for_pca, self, verbose)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfinished\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                 \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_windows_filename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mterminal_text_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m                 \u001b[0msorting_attempts\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'MNT:\\\\d\\\\freelance-work\\\\catalyst-neuro\\\\hussaini-lab-to-nwb\\\\example_data_raw\\\\data_5s\\\\axona_raw_5s_t1_terminal.tx\\\\mnt\\\\d\\\\freelance-work\\\\catalyst-neuro\\\\hussaini-lab-to-nwb\\\\example_data_raw\\\\data_5s\\\\axona_raw_5s_T1_terminal.txt'"
     ]
    }
   ],
   "source": [
    "for i, current_basename in enumerate(basenames):\n",
    "    print('Analyzing set_file %d/%d: ' % (i+1, len(basenames)))\n",
    "    \n",
    "    if whiten != 'true' and automate_threshold:\n",
    "        # then you decided you want to automatically get the threshold from the .set file\n",
    "        set_filename = '%s.set' % os.path.join(directory, current_basename)\n",
    "        detect_threshold = int(get_setfile_parameter('threshold', set_filename))\n",
    "    \n",
    "    print('Using the following detect_threshold: %.2f' % (float(detect_threshold)))\n",
    "        \n",
    "    convert_bin_mountainsort(directory, current_basename, whiten=whiten, \n",
    "                             detect_interval=detect_interval,\n",
    "                             detect_sign=detect_sign, \n",
    "                             detect_threshold=detect_threshold, \n",
    "                             freq_min=freq_min,\n",
    "                             freq_max=freq_max, mask_threshold=mask_threshold, \n",
    "                             masked_chunk_size=masked_chunk_size,\n",
    "                             mask_num_write_chunks=mask_num_write_chunks, \n",
    "                             clip_size=clip_size, \n",
    "                             mask=mask,\n",
    "                             num_features=num_features,\n",
    "                             max_num_clips_for_pca=max_num_clips_for_pca,\n",
    "                             pre_spike=pre_spike, post_spike=post_spike,\n",
    "                             notch_filter=notch_filter, self=self)\n",
    "    \n",
    "    print('-------------------')\n",
    "    \n",
    "print('Finished Analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from BinMSGUI.core.mdaSort import (\n",
    "    get_ubuntu_path, get_windows_filename, sort_finished\n",
    ")\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Errors seem to be related to the get_windows_filename and get_ubuntu_path functions. I don't quite see why I need them anyway, let's try without. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-04-27 11:01:17]: Sorting the following file: /mnt/d/freelance-work/catalyst-neuro/hussaini-lab-to-nwb/example_data_raw/data_5s/axona_raw_5s_T1_filt.mda!\n",
      "ml-run-process ms4_geoff.sort --inputs filt_fname:/mnt/d/freelance-work/catalyst-neuro/hussaini-lab-to-nwb/example_data_raw/data_5s/axona_raw_5s_T1_filt.mda --outputs firings_out:/mnt/d/freelance-work/catalyst-neuro/hussaini-lab-to-nwb/example_data_raw/data_5s/axona_raw_5s_T1_firings.mda pre_out_fname:/mnt/d/freelance-work/catalyst-neuro/hussaini-lab-to-nwb/example_data_raw/data_5s/axona_raw_5s_T1_pre.mda metrics_out_fname:/mnt/d/freelance-work/catalyst-neuro/hussaini-lab-to-nwb/example_data_raw/data_5s/axona_raw_5s_T1_metrics.json masked_out_fname:/mnt/d/freelance-work/catalyst-neuro/hussaini-lab-to-nwb/example_data_raw/data_5s/axona_raw_5s_T1_masked.mda --parameters freq_min:300 freq_max:6000 samplerate:48000 detect_sign:0 adjacency_radius:-1 detect_threshold:3 detect_interval:10 clip_size:50 firing_rate_thresh:0.05 isolation_thresh:0.95 noise_overlap_thresh:0.03 peak_snr_thresh:1.5 mask_artifacts:true mask_chunk_size:2400 mask_threshold:6 mask_num_write_chunks:(100,) num_workers:8 whiten:true num_features:10 max_num_clips_for_pca:1000 >> /mnt/d/freelance-work/catalyst-neuro/hussaini-lab-to-nwb/example_data_raw/data_5s/axona_raw_5s_T1_terminal.txt\n"
     ]
    }
   ],
   "source": [
    "tint_fullpath = os.path.join(directory, current_basename)\n",
    "whiten='true'\n",
    "detect_interval=10\n",
    "detect_sign=0\n",
    "detect_threshold=3\n",
    "freq_min=300\n",
    "freq_max=6000\n",
    "mask_threshold=6\n",
    "masked_chunk_size=None\n",
    "mask_num_write_chunks=100,\n",
    "clip_size=50\n",
    "mask=True\n",
    "num_features=10\n",
    "max_num_clips_for_pca=1000\n",
    "self=None\n",
    "verbose=True\n",
    "\n",
    "if mask:\n",
    "    mask = 'true'\n",
    "else:\n",
    "    mask = 'false'\n",
    "\n",
    "tint_basename = os.path.basename(tint_fullpath)\n",
    "\n",
    "set_filename = '%s.set' % tint_fullpath\n",
    "\n",
    "filt_fnames = [os.path.join(directory, file) for file in os.listdir(\n",
    "    directory) if '_filt.mda' in file if tint_basename in file]\n",
    "\n",
    "for file in filt_fnames:\n",
    "\n",
    "    msg = '[%s %s]: Sorting the following file: %s!' % \\\n",
    "          (str(datetime.datetime.now().date()),\n",
    "           str(datetime.datetime.now().time())[:8], file)\n",
    "\n",
    "    if self:\n",
    "        self.LogAppend.myGUI_signal_str.emit(msg)\n",
    "    else:\n",
    "        print(msg)\n",
    "\n",
    "    mda_basename = os.path.splitext(file)[0]\n",
    "    mda_basename = mda_basename[:find_sub(mda_basename, '_')[-1]]\n",
    "\n",
    "    firings_out = mda_basename + '_firings.mda'\n",
    "\n",
    "    if whiten == 'true':\n",
    "        pre_out_fname = mda_basename + '_pre.mda'\n",
    "    else:\n",
    "        pre_out_fname = None\n",
    "\n",
    "    if mask == 'true':\n",
    "        masked_out_fname = mda_basename + '_masked.mda'\n",
    "    else:\n",
    "        masked_out_fname = None\n",
    "\n",
    "    metrics_out_fname = mda_basename + '_metrics.json'\n",
    "\n",
    "    # check if these outputs have already been created, skip if they have\n",
    "    existing_files = 0\n",
    "    output_files = [masked_out_fname, firings_out, pre_out_fname, metrics_out_fname]\n",
    "    for outfile in output_files:\n",
    "        if outfile is not None:\n",
    "            if os.path.exists(outfile):\n",
    "                existing_files += 1\n",
    "\n",
    "    if existing_files == len(output_files):\n",
    "        msg = '[%s %s]: The following file has already been sorted: %s, skipping sort!#Red' % \\\n",
    "              (str(datetime.datetime.now().date()),\n",
    "               str(datetime.datetime.now().time())[:8], file)\n",
    "\n",
    "        if self:\n",
    "            self.LogAppend.myGUI_signal_str.emit(msg)\n",
    "        else:\n",
    "            print(msg)\n",
    "        continue\n",
    "\n",
    "    terminal_text_filename = mda_basename + '_terminal.txt'\n",
    "\n",
    "    filt_fname = file\n",
    "\n",
    "    Fs = int(get_setfile_parameter('rawRate', set_filename))\n",
    "\n",
    "    if masked_chunk_size is None:\n",
    "        masked_chunk_size = int(Fs/20)\n",
    "\n",
    "    sorting = True\n",
    "    sorting_attempts = 0\n",
    "\n",
    "    if os.path.exists(terminal_text_filename):\n",
    "        os.remove(terminal_text_filename)\n",
    "\n",
    "    while sorting:\n",
    "\n",
    "        run_sort(filt_fname=filt_fname,\n",
    "                 pre_out_fname=pre_out_fname,\n",
    "                 metrics_out_fname=metrics_out_fname,\n",
    "                 firings_out=firings_out,\n",
    "                 masked_out_fname=masked_out_fname,\n",
    "                 samplerate=Fs,\n",
    "                 detect_interval=detect_interval,\n",
    "                 detect_sign=detect_sign,\n",
    "                 detect_threshold=detect_threshold,\n",
    "                 freq_min=freq_min,\n",
    "                 freq_max=freq_max,\n",
    "                 mask_threshold=mask_threshold,\n",
    "                 mask_chunk_size=masked_chunk_size,\n",
    "                 mask_num_write_chunks=mask_num_write_chunks,\n",
    "                 whiten=whiten,\n",
    "                 mask_artifacts=mask,\n",
    "                 clip_size=clip_size,\n",
    "                 num_features=num_features,\n",
    "                 max_num_clips_for_pca=max_num_clips_for_pca,\n",
    "                 terminal_text_filename=terminal_text_filename,\n",
    "                 verbose=verbose)\n",
    "\n",
    "        # wait for the sort to finish before continuing\n",
    "        finished, sort_code = sort_finished(terminal_text_filename)\n",
    "\n",
    "        if sorting_attempts >= 5:\n",
    "            # we've tried to sort a bunch of times, doesn't seem to work\n",
    "            sorting = False\n",
    "\n",
    "        elif 'Abort' in sort_code:\n",
    "            # there's a problem with the sort,\n",
    "            sorting = False\n",
    "            msg = '[%s %s]: There was an error sorting the following file, consult terminal text file: %s!#Red' % \\\n",
    "                  (str(datetime.datetime.now().date()),\n",
    "                   str(datetime.datetime.now().time())[:8], filt_fname)\n",
    "\n",
    "            if self:\n",
    "                self.LogAppend.myGUI_signal_str.emit(msg)\n",
    "            else:\n",
    "                print(msg)\n",
    "\n",
    "        elif not finished:\n",
    "            time.sleep(0.5)\n",
    "            os.remove(terminal_text_filename)\n",
    "            sorting_attempts += 1\n",
    "\n",
    "        else:\n",
    "            sorting = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ml-run-process ms4_geoff.sort --inputs filt_fname:/mnt/mnt/d/freelance-work/catalyst-neuro/hussaini-lab-to-nwb/example_data_raw/data_5s/axona_raw_5s_t1_filt.md/mnt/d/freelance-work/catalyst-neuro/hussaini-lab-to-nwb/example_data_raw/data_5s/axona_raw_5s_T1_filt.mda --outputs firings_out:/mnt/mnt/d/freelance-work/catalyst-neuro/hussaini-lab-to-nwb/example_data_raw/data_5s/axona_raw_5s_t1_firings.md/mnt/d/freelance-work/catalyst-neuro/hussaini-lab-to-nwb/example_data_raw/data_5s/axona_raw_5s_T1_firings.mda pre_out_fname:/mnt/mnt/d/freelance-work/catalyst-neuro/hussaini-lab-to-nwb/example_data_raw/data_5s/axona_raw_5s_t1_pre.md/mnt/d/freelance-work/catalyst-neuro/hussaini-lab-to-nwb/example_data_raw/data_5s/axona_raw_5s_T1_pre.mda metrics_out_fname:/mnt/mnt/d/freelance-work/catalyst-neuro/hussaini-lab-to-nwb/example_data_raw/data_5s/axona_raw_5s_t1_metrics.jso/mnt/d/freelance-work/catalyst-neuro/hussaini-lab-to-nwb/example_data_raw/data_5s/axona_raw_5s_T1_metrics.json masked_out_fname:/mnt/mnt/d/freelance-work/catalyst-neuro/hussaini-lab-to-nwb/example_data_raw/data_5s/axona_raw_5s_t1_masked.md/mnt/d/freelance-work/catalyst-neuro/hussaini-lab-to-nwb/example_data_raw/data_5s/axona_raw_5s_T1_masked.mda --parameters freq_min:300 freq_max:6000 samplerate:48000 detect_sign:0 adjacency_radius:-1 detect_threshold:3 detect_interval:10 clip_size:50 firing_rate_thresh:0.05 isolation_thresh:0.95 noise_overlap_thresh:0.03 peak_snr_thresh:1.5 mask_artifacts:true mask_chunk_size:2400 mask_threshold:6 mask_num_write_chunks:(100,) num_workers:8 whiten:true num_features:10 max_num_clips_for_pca:1000 >> /mnt/mnt/d/freelance-work/catalyst-neuro/hussaini-lab-to-nwb/example_data_raw/data_5s/axona_raw_5s_t1_terminal.tx/mnt/d/freelance-work/catalyst-neuro/hussaini-lab-to-nwb/example_data_raw/data_5s/axona_raw_5s_T1_terminal.txt\n"
     ]
    }
   ],
   "source": [
    "run_sort(filt_fname=filt_fname,\n",
    "         pre_out_fname=pre_out_fname,\n",
    "         metrics_out_fname=metrics_out_fname,\n",
    "         firings_out=firings_out,\n",
    "         masked_out_fname=masked_out_fname,\n",
    "         samplerate=Fs,\n",
    "         detect_interval=detect_interval,\n",
    "         detect_sign=detect_sign,\n",
    "         detect_threshold=detect_threshold,\n",
    "         freq_min=freq_min,\n",
    "         freq_max=freq_max,\n",
    "         mask_threshold=mask_threshold,\n",
    "         mask_chunk_size=masked_chunk_size,\n",
    "         mask_num_write_chunks=mask_num_write_chunks,\n",
    "         whiten=whiten,\n",
    "         mask_artifacts=mask,\n",
    "         clip_size=clip_size,\n",
    "         num_features=num_features,\n",
    "         max_num_clips_for_pca=max_num_clips_for_pca,\n",
    "         terminal_text_filename=terminal_text_filename,\n",
    "         verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
